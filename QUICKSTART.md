# üöÄ Quick Start - Anti-Detection Crawling

## TL;DR (Too Long, Didn't Read)

### Crawl nh·ªè (<20 codes):
```python
results = await crawl_multiple_tax_codes(tax_codes)
```
‚úÖ Nhanh, an to√†n, kh√¥ng c·∫ßn config g√¨ th√™m

### Crawl l·ªõn (>100 codes):
```python
results = await crawl_multiple_tax_codes(
    tax_codes,
    batch_size=2,      # Nh·ªè h∆°n = an to√†n h∆°n
    delay_range=(3, 7) # Delay d√†i h∆°n = √≠t b·ªã ch·∫∑n h∆°n
)
```
‚úÖ Ch·∫≠m nh∆∞ng an to√†n

### B·ªã ch·∫∑n r·ªìi?
```python
results = await crawl_multiple_tax_codes(
    tax_codes,
    batch_size=1,       # T·ª´ng c√°i m·ªôt
    delay_range=(5, 10) # Ch·ªù l√¢u gi·ªØa requests
)
```
‚úÖ R·∫•t ch·∫≠m nh∆∞ng r·∫•t an to√†n

## Cheat Sheet

| Scenario | batch_size | delay_range | Speed | Safety |
|----------|------------|-------------|-------|--------|
| Test (5-10 codes) | 3 | (1, 2) | ‚ö°‚ö°‚ö° Fast | ‚úÖ OK |
| Normal (<50 codes) | 3 | (2, 5) | ‚ö°‚ö° Medium | ‚úÖ‚úÖ Good |
| Large (100+ codes) | 2 | (3, 7) | ‚ö° Slow | ‚úÖ‚úÖ‚úÖ Safe |
| Very Large (500+) | 1-2 | (5, 10) | üêå Very Slow | üõ°Ô∏è Very Safe |
| Got Blocked | 1 | (10, 15) | üêåüêå Ultra Slow | üõ°Ô∏èüõ°Ô∏è Ultra Safe |

## Time Estimation

```
∆Ø·ªõc t√≠nh th·ªùi gian ‚âà (s·ªë_codes / batch_size) √ó delay_trung_b√¨nh

V√≠ d·ª•:
- 100 codes, batch=3, delay=3.5s ‚Üí (100/3) √ó 3.5 ‚âà 117s ‚âà 2 ph√∫t
- 500 codes, batch=2, delay=7.5s ‚Üí (500/2) √ó 7.5 ‚âà 1875s ‚âà 31 ph√∫t
```

## Code Examples

### Example 1: Quick test
```python
import asyncio
from crawler import crawl_multiple_tax_codes

tax_codes = ["0318735609", "5801554055", "0111265890"]

results = await crawl_multiple_tax_codes(tax_codes)
# ‚Üí ~10 seconds
```

### Example 2: Production use
```python
# Read from CSV
import pandas as pd
df = pd.read_csv("tax_codes.csv", dtype=str)
tax_codes = df['dinh_danh_doanh_nghiep'].tolist()

# Crawl with safe settings
results = await crawl_multiple_tax_codes(
    tax_codes,
    batch_size=2,
    delay_range=(3, 6)
)

# Save results
results_df = pd.DataFrame(results)
results_df.to_csv("results.csv", index=False, encoding='utf-8-sig')
```

### Example 3: Handle errors
```python
results = await crawl_multiple_tax_codes(tax_codes)

# Check results
success_count = sum(1 for r in results if 'T√™n' in r)
error_count = len(results) - success_count

print(f"Success: {success_count}/{len(results)}")
print(f"Errors: {error_count}")

# Get failed codes
failed_codes = [r['MST'] for r in results if 'T√™n' not in r]
if failed_codes:
    print(f"Failed codes: {failed_codes}")
    # Retry with safer settings
    retry_results = await crawl_multiple_tax_codes(
        failed_codes,
        batch_size=1,
        delay_range=(5, 10)
    )
```

## Web Interface

1. Go to http://localhost:8000
2. Upload CSV with column `dinh_danh_doanh_nghiep`
3. Click "X·ª≠ l√Ω file CSV"
4. Wait (check console for progress)
5. Download results

**Auto settings:**
- ‚â§20 codes ‚Üí batch_size=3 (fast)
- >20 codes ‚Üí batch_size=2 (safer)

## Troubleshooting

### ‚ùå Getting captcha
‚Üí Gi·∫£m batch_size v√† tƒÉng delay

### ‚ùå Timeout errors
‚Üí Ki·ªÉm tra internet, th·ª≠ l·∫°i sau

### ‚ùå No data returned
‚Üí Check m√£ s·ªë thu·∫ø c√≥ ƒë√∫ng kh√¥ng

### ‚ùå Process too slow
‚Üí TƒÉng batch_size (nh∆∞ng r·ªßi ro cao h∆°n)

## Best Practices

‚úÖ **DO:**
- Test nh·ªè tr∆∞·ªõc (5-10 codes)
- Monitor console logs
- Save results incrementally
- Crawl off-peak hours (night time)
- Use default settings first

‚ùå **DON'T:**
- Crawl batch_size > 5
- Set delay < 1 second
- Crawl thousands without testing
- Ignore error messages
- Retry immediately after blocking

## Need Help?

1. Check [ANTI_DETECTION.md](ANTI_DETECTION.md) for details
2. Check [ANTI_DETECTION_SUMMARY.md](ANTI_DETECTION_SUMMARY.md) for summary
3. Check console logs for errors
4. Try safer settings (batch_size=1, longer delays)

---
**Last updated**: 31/10/2025

